{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#neural_networks.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from utils import softmax_cross_entropy, add_momentum, data_loader_mnist, predict_label, DataSplit\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "# 1. One linear Neural Network layer with forward and backward steps\n",
    "\n",
    "### Modules ###\n",
    "\n",
    "class linear_layer:\n",
    "\n",
    "    def __init__(self, input_D, output_D):\n",
    "\n",
    "        self.params = dict()\n",
    "   \n",
    "        self.params['W'] = np.random.normal(0, 0.1, size = (input_D, output_D))        \n",
    "        self.params['b'] = np.random.normal(0, 0.1, size = (1, output_D))\n",
    "                \n",
    "        self.gradient = dict()\n",
    "        \n",
    "        self.gradient['W'] = np.zeros(shape = (input_D,output_D))\n",
    "        self.gradient['b'] = np.zeros(shape = (1,output_D))\n",
    "                \n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        forward_output = np.matmul(X, self.params['W']) + self.params['b']        \n",
    "                \n",
    "        return forward_output\n",
    "\n",
    "    def backward(self, X, grad):\n",
    "\n",
    "        \n",
    "        self.gradient['W'] = np.matmul(X.T, grad)\n",
    "        self.gradient['b'] = np.sum(grad, axis = 0).reshape((1,-1))        \n",
    "        \n",
    "        backward_output = np.matmul(grad, self.params['W'].T)\n",
    "                \n",
    "        return backward_output\n",
    "\n",
    "\n",
    "# 2. ReLU Activation\n",
    "\n",
    "\n",
    "class relu:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        forward_output = np.maximum(0, X)\n",
    "        \n",
    "        #raise NotImplementedError(\"Not Implemented function: forward, class: relu\")\n",
    "        return forward_output\n",
    "\n",
    "    def backward(self, X, grad):\n",
    "\n",
    "\n",
    "        relu_deriv = ((X > 0).astype(int))        \n",
    "        backward_output = grad*relu_deriv\n",
    "        \n",
    "        #raise NotImplementedError(\"Not Implemented function: backward, class: relu\")\n",
    "        return backward_output\n",
    "\n",
    "\n",
    "# 3. tanh Activation\n",
    "\n",
    "class tanh:\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        forward_output = np.tanh(X)\n",
    "        \n",
    "        #raise NotImplementedError(\"Not Implemented function: forward, class: tanh\")\n",
    "        return forward_output\n",
    "\n",
    "    def backward(self, X, grad):\n",
    "\n",
    "        backward_output = grad*(1 - pow(np.tanh(X),2))\n",
    "        \n",
    "        #raise NotImplementedError(\"Not Implemented function: backward, class: tanh\")\n",
    "        return backward_output\n",
    "\n",
    "\n",
    "# 4. Dropout\n",
    "\n",
    "class dropout:\n",
    "\n",
    "\n",
    "    def __init__(self, r):\n",
    "        self.r = r\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, X, is_train):\n",
    "\n",
    "        if is_train:\n",
    "            self.mask = (np.random.uniform(0.0, 1.0, X.shape) >= self.r).astype(float) * (1.0 / (1.0 - self.r))\n",
    "        else:\n",
    "            self.mask = np.ones(X.shape)\n",
    "        forward_output = np.multiply(X, self.mask)\n",
    "        return forward_output\n",
    "\n",
    "    def backward(self, X, grad):\n",
    "\n",
    "        backward_output = grad*self.mask\n",
    "        \n",
    "        #raise NotImplementedError(\"Not Implemented function: backward, class: dropout\")\n",
    "        return backward_output\n",
    "\n",
    "\n",
    "\n",
    "# 5. Mini-batch Gradient Descent Optimization\n",
    "\n",
    "\n",
    "def miniBatchGradientDescent(model, momentum, _lambda, _alpha, _learning_rate):\n",
    "\n",
    "    for module_name, module in model.items():\n",
    "\n",
    "        # check if a module has learnable parameters\n",
    "        if hasattr(module, 'params'):\n",
    "            for key, _ in module.params.items():\n",
    "                g = module.gradient[key] + _lambda * module.params[key]\n",
    "\n",
    "                if _alpha > 0.0:\n",
    "                    \n",
    "                    momentum[module_name + '_' + key] = _alpha * momentum[module_name + '_' + key] - _learning_rate * g         \n",
    "                    module.params[key] += momentum[module_name + '_' + key]\n",
    "                    \n",
    "                    # raise NotImplementedError(\"Not Implemented function: miniBatchGradientDescent\")\n",
    "                    \n",
    "                else:\n",
    "\n",
    "        \n",
    "                    module.params[key] -= momentum[module_name + '_' + key]\n",
    "                    \n",
    "                    # raise NotImplementedError(\"Not Implemented function: miniBatchGradientDescent\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def main(main_params, optimization_type=\"minibatch_sgd\"):\n",
    "\n",
    "    ### set the random seed ###\n",
    "    np.random.seed(int(main_params['random_seed']))\n",
    "\n",
    "    ### data processing ###\n",
    "    Xtrain, Ytrain, Xval, Yval , _, _ = data_loader_mnist(dataset = main_params['input_file'])\n",
    "    N_train, d = Xtrain.shape\n",
    "    N_val, _ = Xval.shape\n",
    "\n",
    "    index = np.arange(10)\n",
    "    unique, counts = np.unique(Ytrain, return_counts=True)\n",
    "    counts = dict(zip(unique, counts)).values()\n",
    "\n",
    "    trainSet = DataSplit(Xtrain, Ytrain)\n",
    "    valSet = DataSplit(Xval, Yval)\n",
    "\n",
    "    ### building/defining MLP ###\n",
    "\n",
    "    model = dict()\n",
    "    num_L1 = 1000\n",
    "    num_L2 = 10\n",
    "\n",
    "    # experimental setup\n",
    "    num_epoch = int(main_params['num_epoch'])\n",
    "    minibatch_size = int(main_params['minibatch_size'])\n",
    "\n",
    "    # optimization setting: _alpha for momentum, _lambda for weight decay\n",
    "    _learning_rate = float(main_params['learning_rate'])\n",
    "    _step = 10\n",
    "    _alpha = float(main_params['alpha'])\n",
    "    _lambda = float(main_params['lambda'])\n",
    "    _dropout_rate = float(main_params['dropout_rate'])\n",
    "    _activation = main_params['activation']\n",
    "\n",
    "\n",
    "    if _activation == 'relu':\n",
    "        act = relu\n",
    "    else:\n",
    "        act = tanh\n",
    "\n",
    "    # create objects (modules) from the module classes\n",
    "    model['L1'] = linear_layer(input_D = d, output_D = num_L1)\n",
    "    model['nonlinear1'] = act()\n",
    "    model['drop1'] = dropout(r = _dropout_rate)\n",
    "    model['L2'] = linear_layer(input_D = num_L1, output_D = num_L2)\n",
    "    model['loss'] = softmax_cross_entropy()\n",
    "\n",
    "    # Momentum\n",
    "    if _alpha > 0.0:\n",
    "        momentum = add_momentum(model)\n",
    "    else:\n",
    "        momentum = None\n",
    "\n",
    "    train_acc_record = []\n",
    "    val_acc_record = []\n",
    "\n",
    "    train_loss_record = []\n",
    "    val_loss_record = []\n",
    "\n",
    "    ### run training and validation ###\n",
    "    for t in range(num_epoch):\n",
    "        print('At epoch ' + str(t + 1))\n",
    "        if (t % _step == 0) and (t != 0):\n",
    "            _learning_rate = _learning_rate * 0.1\n",
    "\n",
    "        idx_order = np.random.permutation(N_train)\n",
    "\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "        train_count = 0\n",
    "\n",
    "        val_acc = 0.0\n",
    "        val_count = 0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        for i in range(int(np.floor(N_train / minibatch_size))):\n",
    "\n",
    "            # get a mini-batch of data\n",
    "            x, y = trainSet.get_example(idx_order[i * minibatch_size : (i + 1) * minibatch_size])\n",
    "\n",
    "            ### forward ###\n",
    "            a1 = model['L1'].forward(x)\n",
    "            h1 = model['nonlinear1'].forward(a1)\n",
    "            d1 = model['drop1'].forward(h1, is_train = True)\n",
    "            a2 = model['L2'].forward(d1)\n",
    "            loss = model['loss'].forward(a2, y)\n",
    "\n",
    "\n",
    "            ### backward ###\n",
    "            grad_a2 = model['loss'].backward(a2, y)\n",
    "\n",
    "            \n",
    "            grad_d1 = model['L2'].backward(d1, grad_a2)            \n",
    "            \n",
    "            grad_h1 = model['drop1'].backward(h1, grad_d1)\n",
    "            \n",
    "            grad_a1 = model['nonlinear1'].backward(a1, grad_h1)\n",
    "            \n",
    "            #raise NotImplementedError(\"Not Implemented BACKWARD PASS in main()\")\n",
    "\n",
    "\n",
    "            grad_x = model['L1'].backward(x, grad_a1)\n",
    "\n",
    "            ### gradient_update ###\n",
    "            model = miniBatchGradientDescent(model, momentum, _lambda, _alpha, _learning_rate)\n",
    "            \n",
    "        ### Computing training accuracy and obj ###\n",
    "        for i in range(int(np.floor(N_train / minibatch_size))):\n",
    "\n",
    "            x, y = trainSet.get_example(np.arange(i * minibatch_size, (i + 1) * minibatch_size))\n",
    "\n",
    "            ### forward ###\n",
    "\n",
    "            # Pasted from above\n",
    "            a1 = model['L1'].forward(x)\n",
    "            h1 = model['nonlinear1'].forward(a1)\n",
    "            d1 = model['drop1'].forward(h1, is_train = False)\n",
    "            a2 = model['L2'].forward(d1)            \n",
    "            \n",
    "            #raise NotImplementedError(\"Not Implemented COMPUTING TRAINING ACCURACY in main()\")\n",
    "\n",
    "       \n",
    "            loss = model['loss'].forward(a2, y)\n",
    "            train_loss += loss\n",
    "            train_acc += np.sum(predict_label(a2) == y)\n",
    "            train_count += len(y)\n",
    "\n",
    "        train_loss = train_loss\n",
    "        train_acc = train_acc / train_count\n",
    "        train_acc_record.append(train_acc)\n",
    "        train_loss_record.append(train_loss)\n",
    "\n",
    "        print('Training loss at epoch ' + str(t + 1) + ' is ' + str(train_loss))\n",
    "        print('Training accuracy at epoch ' + str(t + 1) + ' is ' + str(train_acc))\n",
    "\n",
    "        ### Computing validation accuracy ###\n",
    "        for i in range(int(np.floor(N_val / minibatch_size))):\n",
    "\n",
    "            x, y = valSet.get_example(np.arange(i * minibatch_size, (i + 1) * minibatch_size))\n",
    "\n",
    "            ### forward ###\n",
    "           \n",
    "            # Pasted from above\n",
    "            a1 = model['L1'].forward(x)\n",
    "            h1 = model['nonlinear1'].forward(a1)\n",
    "            d1 = model['drop1'].forward(h1, is_train = False)\n",
    "            a2 = model['L2'].forward(d1)         \n",
    "            \n",
    "            raise NotImplementedError(\"Not Implemented COMPUTING VALIDATION ACCURACY in main()\")\n",
    "\n",
    "            loss = model['loss'].forward(a2, y)\n",
    "            val_loss += loss\n",
    "            val_acc += np.sum(predict_label(a2) == y)\n",
    "            val_count += len(y)\n",
    "\n",
    "        val_loss_record.append(val_loss)\n",
    "        val_acc = val_acc / val_count\n",
    "        val_acc_record.append(val_acc)\n",
    "\n",
    "        print('Validation accuracy at epoch ' + str(t + 1) + ' is ' + str(val_acc))\n",
    "\n",
    "    # save file\n",
    "    json.dump({'train': train_acc_record, 'val': val_acc_record},\n",
    "              open('MLP_lr' + str(main_params['learning_rate']) +\n",
    "                   '_m' + str(main_params['alpha']) +\n",
    "                   '_w' + str(main_params['lambda']) +\n",
    "                   '_d' + str(main_params['dropout_rate']) +\n",
    "                   '_a' + str(main_params['activation']) +\n",
    "                   '.json', 'w'))\n",
    "\n",
    "    print('Finish running!')\n",
    "    return train_loss_record, val_loss_record\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--random_seed', default=42)\n",
    "    parser.add_argument('--learning_rate', default=0.01)\n",
    "    parser.add_argument('--alpha', default=0.0)\n",
    "    parser.add_argument('--lambda', default=0.0)\n",
    "    parser.add_argument('--dropout_rate', default=0.0)\n",
    "    parser.add_argument('--num_epoch', default=10)\n",
    "    parser.add_argument('--minibatch_size', default=5)\n",
    "    parser.add_argument('--activation', default='relu')\n",
    "    parser.add_argument('--input_file', default='mnist_subset.json')\n",
    "    args = parser.parse_args()\n",
    "    main_params = vars(args)\n",
    "    main(main_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils.py\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This script is adapted and modified based on the assignment of\n",
    "\n",
    "Do not change this script.\n",
    "If our script cannot run your code or the format is improper, your code will not be graded.\n",
    "\"\"\"\n",
    "\n",
    "# Softmax loss and Softmax gradient\n",
    "### Loss functions ###\n",
    "\n",
    "class softmax_cross_entropy:\n",
    "    def __init__(self):\n",
    "        self.expand_Y = None\n",
    "        self.calib_logit = None\n",
    "        self.sum_exp_calib_logit = None\n",
    "        self.prob = None\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        self.expand_Y = np.zeros(X.shape).reshape(-1)\n",
    "        self.expand_Y[Y.astype(int).reshape(-1) + np.arange(X.shape[0]) * X.shape[1]] = 1.0\n",
    "        self.expand_Y = self.expand_Y.reshape(X.shape)\n",
    "\n",
    "        self.calib_logit = X - np.amax(X, axis = 1, keepdims = True)\n",
    "        self.sum_exp_calib_logit = np.sum(np.exp(self.calib_logit), axis = 1, keepdims = True)\n",
    "        self.prob = np.exp(self.calib_logit) / self.sum_exp_calib_logit\n",
    "\n",
    "        forward_output = - np.sum(np.multiply(self.expand_Y, self.calib_logit - np.log(self.sum_exp_calib_logit))) / X.shape[0]\n",
    "        return forward_output\n",
    "\n",
    "    def backward(self, X, Y):\n",
    "        backward_output = - (self.expand_Y - self.prob) / X.shape[0]\n",
    "        return backward_output\n",
    "\n",
    "\n",
    "### Momentum ###\n",
    "\n",
    "def add_momentum(model):\n",
    "    momentum = dict()\n",
    "    for module_name, module in model.items():\n",
    "        if hasattr(module, 'params'):\n",
    "            for key, _ in module.params.items():\n",
    "                momentum[module_name + '_' + key] = np.zeros(module.gradient[key].shape)\n",
    "    return momentum\n",
    "\n",
    "\n",
    "def data_loader_mnist(dataset):\n",
    "    # This function reads the MNIST data and separate it into train, val, and test set\n",
    "    with open(dataset, 'r') as f:\n",
    "        data_set = json.load(f)\n",
    "    train_set, valid_set, test_set = data_set['train'], data_set['valid'], data_set['test']\n",
    "\n",
    "    Xtrain = np.array(train_set[0])\n",
    "    Ytrain = np.array(train_set[1])\n",
    "    Xvalid = np.array(valid_set[0])\n",
    "    Yvalid = np.array(valid_set[1])\n",
    "    Xtest = np.array(test_set[0])\n",
    "    Ytest = np.array(test_set[1])\n",
    "\n",
    "    return Xtrain, Ytrain, Xvalid, Yvalid, Xtest, Ytest\n",
    "\n",
    "\n",
    "def predict_label(f):\n",
    "    # This is a function to determine the predicted label given scores\n",
    "    if f.shape[1] == 1:\n",
    "        return (f > 0).astype(float)\n",
    "    else:\n",
    "        return np.argmax(f, axis=1).astype(float).reshape((f.shape[0], -1))\n",
    "\n",
    "\n",
    "class DataSplit:\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.N, self.d = self.X.shape\n",
    "\n",
    "    def get_example(self, idx):\n",
    "        batchX = np.zeros((len(idx), self.d))\n",
    "        batchY = np.zeros((len(idx), 1))\n",
    "        for i in range(len(idx)):\n",
    "            batchX[i] = self.X[idx[i]]\n",
    "            batchY[i, :] = self.Y[idx[i]]\n",
    "        return batchX, batchY"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
